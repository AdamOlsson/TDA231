{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ledX1hQo5kXC"
   },
   "source": [
    "\n",
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Home Assignment 3 -- Classification** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: K-Nearest-Neighbour, Naive-bayes Classifier, Support Vector Machine, Logistic Regression**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Yuchong, Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 9th May** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Adam Olsson & Gabriel Lindeby, 950418-3170 & 951027-1779, adaolss@student.chalmers.se & gablinde@student.chalmers.se** <br />\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "General guidelines:\n",
    "*   All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
    "*   All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
    "*   Your name, personal number and email address should be specified above.\n",
    "*   All tables and other additional information should be included in this notebook.\n",
    "*   **All the answers for theoretical questions must be filled in the cells created for you with \"Your answer here\" below each question, but feel free to add more cells if needed.**\n",
    "*   Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbqQD9bj6HzP"
   },
   "source": [
    "\n",
    "# Theoretical Questions\n",
    "## 1. K-Nearest-Neighbour Classification (4 pts)\n",
    "### 1.1 Exercise 1 (2 pts)\n",
    "A KNN classifier assigns a test instance the majority class associated with its K nearest training instances. Distance between instances is measured using Euclidean distance. Suppose we have the following training set of positive (+) and negative (-) instances and a single test instance (o). All instances are projected onto a vector space of two real-valued features (X and Y). Answer the following questions. Assume “unweighted” KNN (every nearest neighbor contributes equally to the final vote).\n",
    "\n",
    "![替代文字](https://raw.githubusercontent.com/BruceZHANG6/Imagesforuse/master/knn.png)\n",
    "\n",
    "a) What would be the class assigned to this test instance for K=1, K=3, K=5 and why? (**1 pt**)\n",
    "\n",
    "\n",
    "b) What will be the maxinum value of K you think in this case? Why? (**1 pt**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0xnH14a0rsj"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "a) For K=1 and K=3 the test instance will be labled as (-) because a majority of the neighbors are from the (-) class. However, when K=5 the majority of the neighbors are from the (+) class and therefor the test instance is labled as (+).\n",
    "\n",
    "b) The largest K value would be K=9 since this still allows the (-) class to get a majority. K=10 would also be possible but in the cases where all 5 (-) instances are neighbors to the test instance result in a 50/50 chance of a test instance being assigned to the (-) class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqgZRVS80xr7"
   },
   "source": [
    "### 1.2 Exercise 2 (2 pts)\n",
    "Consider 5 data points:\n",
    "\n",
    "$$\\{({0},{1}), ({-1},{0})\\}∈ Class1,$$ \n",
    "\n",
    "$$\\{({1},{0}), ({0},{-1}), (-\\frac{1}{2}, \\frac{1}{2})\\}∈ Class2.$$\n",
    "\n",
    "Consider two test data points:\n",
    "\n",
    "$$(-\\frac{3}{4}, \\frac{3}{4})∈ Class1, (\\frac{1}{2}, \\frac{1}{2})∈ Class2$$\n",
    "\n",
    "Compute the probability of error based on k-nearest neighbor rule when $ K=\\{1, 2, 3, 4, 5\\}$ and explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [0,-1, 1,  0, -0.5, -0.75, 0.5]\n",
    "y = [1, 0, 0, -1,  0.5,  0.75, 0.5]\n",
    "label = [1, 1, -1, -1, -1, 0, 2]\n",
    "colors = ['red','blue', 'green', 'black']\n",
    "\n",
    "plt.scatter(x, y, c=label, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK-7HuAB0ztS"
   },
   "source": [
    "### Your answer here:\n",
    "For plot:\n",
    "\n",
    "Train:\n",
    "Green dots class 1.\n",
    "Red dots class 2.\n",
    "\n",
    "Test:\n",
    "Blue dot belongs to class 1.\n",
    "Black dot belongs to class 2.\n",
    "\n",
    "K = 1: P(Blue misclassified)* P(Black misclassified) = 1 * 0.5 = 0.5\n",
    "Blue dot will be misclassified because the closest neighbour belongs to other class.\n",
    "Black dot will be has a 50% chance of being misclassified since it is in the middle between two points and since K = 1 it will choose at random which of the two points to consider as a neighbour. Since one of the points is same class and the other opposite class we'll have a 50/50 shot of misclassification.\n",
    "\n",
    "K = 2: P(Blue misclassified)* P(Black misclassified) = 0.5 * 0.5 = 0.25.\n",
    "Both points have an even split between the classes within their neighbours. The classifier will at random choose which class to label the test points to, thus both points have a 50/50 shot of misclassification.\n",
    "\n",
    "K = 3: P(Blue misclassified)* P(Black misclassified) = 0 * 0 = 0.\n",
    "For both points, a majority of the neighbours belongs to the same class so the classifier will label both correctly.\n",
    "\n",
    "\n",
    "K = 4: P(Blue misclassified)* P(Black misclassified) = 0.5 * 0 = 0. \n",
    "The blue point have a tie between the classes within its neighbourhood and therefor it will choose at random which means a 50/50 shot of error. Additionally, for any higher K values the blue point will always be misclassified since there only are 2 points belonging to the same class as the blue point. Class 1 will never have a majority and therefor no new point will be assigned to that class. The black point has a 0 chance of misclassification since in its neighbourhood there is a majority towards its class.\n",
    "\n",
    "K = 5: P(Blue misclassified)* P(Black misclassified) = 1 * 0 = 0. \n",
    "All points of the entire data set is now part of the classification. Since class 2 is larger than class 1, both test points will be assigned to class 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8t9u_kDpgTD2"
   },
   "source": [
    "## 2. [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "### Exercise 1 (3 pts)\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 0) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 1),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''? **(1 pt)**\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'') **(2 pts)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEEeDnaN1Ikp"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "|     isRich  |  C | NC| P(C)|P(NC)|\n",
    "|-------------|----|---|-----|-----|\n",
    "| 0           | 1  | 3 | 1/4 | 3/4 |\n",
    "| 1           | 3  | 1 | 3/4 | 1/4 |\n",
    "\n",
    "|  isMarried  |  C | NC| P(C)|P(NC)|\n",
    "|-------------|----|---|-----|-----|\n",
    "| 0           | 2  | 3 | 2/4 | 3/4 |\n",
    "| 1           | 2  | 1 | 2/4 | 1/4 |\n",
    "\n",
    "|isHealthy | C| NC | P(C) | P(NC) |\n",
    "|:-------|-:|---:|-----:|------:| \n",
    "|0       | 2|   2|   2/4|    2/4|\n",
    "|1       | 2|   2|   2/4|    2/4|\n",
    "\n",
    "|  isContent  |  amount | P( )|\n",
    "|-------------|---------|----|\n",
    "| No          | 4       | 4/8|\n",
    "| Yes         | 4       | 4/8|\n",
    "\n",
    "The probabilites was found by looking at how likely each feature was in relation to the other features in the training data. \n",
    "\n",
    "1) \n",
    "\n",
    "$$P(c=1 | (0,1,1)) = P(c=1 | rich=0)P(c=1 | married=1)P(c=1 | healthy=1)P(c=1) = $$ \n",
    "$$\\frac{1}{4}*\\frac{1}{2}*\\frac{1}{2}*\\frac{1}{2} = \\frac{1}{32}$$\n",
    "\n",
    "$$P(c=0 | (0,1,1)) = \\frac{3}{4}*\\frac{1}{4}*\\frac{1}{2}*\\frac{1}{2} = \\frac{3}{64}$$\n",
    "\n",
    "$$ P(c=1 | (0,1,1)) = \\frac{\\frac{1}{32}}{\\frac{1}{32}+ \\frac{3}{64}} = 0.4 = 40\\%$$\n",
    "\n",
    "2) \n",
    "\n",
    "$$P(c = 1 | (0,1,0)) + P(c = 1 | (0,1,1)) = \\frac{1}{4}\\frac{1}{2}\\frac{1}{2}\\frac{1}{2} + \\frac{1}{4}\\frac{1}{2}\\frac{1}{2}\\frac{1}{2} = \\frac{2}{32}$$\n",
    "\n",
    "$$P(c = 0 | (0,1,0)) + P(c = 0 | (0,1,1)) = \\frac{3}{4}\\frac{1}{4}\\frac{1}{2}\\frac{1}{2} + \\frac{3}{4}\\frac{1}{4}\\frac{1}{2}\\frac{1}{2} = \\frac{6}{64}$$\n",
    "\n",
    "$$P(c=1 | (rich=0, married=1)) = \\frac{\\frac{2}{32}}{\\frac{2}{32} + \\frac{6}{64}} = 0.4 = 40\\%$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FkqFQkN1LWx"
   },
   "source": [
    "### Exercise  2 (3 pts)\n",
    "We now consider real valued data i.e. $x \\in \\mathbb{R}^2$.\n",
    "#### a. (1 pt)\n",
    "Assume that the class conditional density is **spherical** Gaussian, and both classes have equal fixed prior i.e. $p(y= \\pm 1) = 0.5$. Write the expression for the **naive Bayes** classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\ \\tag{1}\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $i$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\mu_{i}, \\sigma^{2}_{i})\n",
    "$$\n",
    "\n",
    "where $\\mu_{i} \\in \\mathbb{R}^2$ and $\\sigma^{2}_{i}\\in \\mathbb{R}$.\n",
    "\n",
    "***Hint***: Derive the expressions of MLE for parameters in terms of training-data. Then express eq.1 in terms of those estimates. \n",
    "\n",
    "#### b. (2 pts)\n",
    "Derive the MLE expression for parameters when the covariance matrix is not diagonal, i.e, Covariance matrix has 4 unknown parameters. This is done to alleviate \"naive\" assumption, since now feature components are no longer independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nyg8jlaj1Nb8"
   },
   "source": [
    "### Your answer here:\n",
    "A)\n",
    "\n",
    "$$ln(L) = \\prod^{N_{c}}N(\\mu_{c}, \\Sigma_{c}) = $$\n",
    "\n",
    "$$ \\sum^{N_{c}}_{n=1}ln(\\frac{1}{(2*\\pi)^{\\frac{D}{2}}|\\Sigma_{c}|^{\\frac{1}{2}}}\\exp(-\\frac{1}{2}(X_{n}-\\mu_{c})^T\\Sigma^{-1}_{c}(X_{n}-\\mu_{c})))$$\n",
    "\n",
    "Removing constants that will disappear in derivate:\n",
    "\n",
    "$$N_{c}ln(\\frac{1}{|\\Sigma_{c}|^{\\frac{1}{2}}}) - \\frac{1}{2}\\sum^{N_{c}}_{n=1}(X_{n}-\\mu_{c})^T\\Sigma^{-1}_{c}(X_{n}-\\mu_{c}) = $$\n",
    "\n",
    "$$\\frac{N_{c}}{2}ln(|\\Sigma_{c}|^{-1}) - \\frac{1}{2}\\sum^{N_{c}}_{n=1}(X_{n}-\\mu_{c})^T\\Sigma^{-1}_{c}(X_{n}-\\mu_{c})$$\n",
    "\n",
    "Using the following rules and trace we can rewrite as:\n",
    "\n",
    "(1) trace(a) = a if a is single value\n",
    "\n",
    "(2) trace(AB) = trace(BA)\n",
    "\n",
    "$$\\frac{N_{c}}{2}ln(|\\Sigma_{c}|^{-1}) - \\frac{1}{2}\\sum^{N_{c}}_{n=1}trace((X_{n}-\\mu_{c})^T\\Sigma^{-1}_{c}(X_{n}-\\mu_{c})) = $$\n",
    "\n",
    "$$\\frac{N_{c}}{2}ln(|\\Sigma_{c}|^{-1}) - \\frac{1}{2}\\sum^{N_{c}}_{n=1}trace((X_{n}-\\mu_{c})^T(X_{n}-\\mu_{c})\\Sigma^{-1}_{c}) $$\n",
    "\n",
    "Taking the derivative w.r.t $\\Sigma^{-1}_{c}:$\n",
    "$$\\frac{\\partial ln(L)}{\\partial \\Sigma^{-1}_{c}} = \\frac{N_{c}}{2}\\Sigma_{c}^{--T} - \\frac{1}{2}\\sum^{N_{c}}_{n=1}((X_{n}-\\mu_{c})^T(X_{n}-\\mu_{c}))^{T} = $$\n",
    "\n",
    "$$ \\frac{N_{c}}{2}\\Sigma_{c} - \\frac{1}{2}\\sum^{N_{c}}_{n=1}(X_{n}-\\mu_{c})(X_{n}-\\mu_{c})^T $$\n",
    "\n",
    "Solving for $\\Sigma_{c} = 0$:\n",
    "\n",
    "$$\\frac{N_{c}}{2}\\Sigma_{c} - \\frac{1}{2}\\sum^{N_{c}}_{n=1}(X_{n}-\\mu_{c})(X_{n}-\\mu_{c})^T  = 0$$\n",
    "\n",
    "$$N_{c}\\Sigma_{c} = \\sum^{N_{c}}_{n=1}(X_{n}-\\mu_{c})(X_{n}-\\mu_{c})^T$$\n",
    "\n",
    "$$\\Sigma_{c} = \\frac{1}{N_{c}}\\sum^{N_{c}}_{n=1}(X_{n}-\\mu_{c})(X_{n}-\\mu_{c})^T $$\n",
    "\n",
    "When solving for $\\mu$ we'll do a tiny change to our loglikelihood function:\n",
    "\n",
    "$$\\frac{N_{c}}{2}ln(|\\Sigma_{c}|^{-1}) - \\frac{1}{2}\\sum^{N_{c}}_{n=1}(X_{n}-\\mu_{c})^T\\Sigma^{-1}_{c}(X_{n}-\\mu_{c}) = $$\n",
    "\n",
    "$$-\\frac{N_{c}}{2}ln(|\\Sigma_{c}|) - \\frac{1}{2}\\sum^{N_{c}}_{n=1}(X_{n}-\\mu_{c})^T\\Sigma^{-1}_{c}(X_{n}-\\mu_{c})$$\n",
    "\n",
    "Now taking the derivative w.r.t $\\mu_{c}$: (using the rule $\\frac{\\partial w^TAw}{\\partial w} = 2Aw$)\n",
    "\n",
    "\n",
    "$$\\frac{\\partial ln(L)}{\\partial \\mu_{c}} = -\\frac{1}{2}\\sum^{N_{c}}_{n=1}2\\Sigma^{-1}_{c}(X_{n}-\\mu_{c}) = $$\n",
    "\n",
    "$$ \\sum^{N_{c}}_{n=1}\\Sigma^{-1}_{c}(\\mu_{c}-X_{n}) $$\n",
    "\n",
    "Because $\\Sigma^{-1}_{c}$ is positive definite:\n",
    "\n",
    "$$ \\sum^{N_{c}}_{n=1}\\Sigma^{-1}_{c}(\\mu_{c}-X_{n}) = $$\n",
    "$$ \\frac{\\partial ln(L)}{\\partial \\mu_{c}} = \\sum^{N_{c}}_{n=1}(\\mu_{c}-X_{n}) $$\n",
    "\n",
    "Now finally solving for $\\mu_{c} = 0$:\n",
    "\n",
    "$$ \\sum^{N_{c}}_{n=1}(\\mu_{c}-X_{n})  = 0$$\n",
    "$$ N_{c}\\mu_{c}-\\sum^{N_{c}}_{n=1}X_{n}  = 0$$\n",
    "$$ \\mu_{c}= \\frac{1}{N_{c}}\\sum^{N_{c}}_{n=1}X_{n}$$\n",
    "\n",
    "So back to the task:\n",
    "\n",
    "$$P(T_{new} = c | x_{new}, X, t) = \\frac{p(x_{new}|T_{new}=c,X,t)P(T_{new}=c|X,t)}{\\sum^{C}_{c'=1}p(x_{new}|T_{new}=c',X,t)P(T_{new}=c'|X,t)}$$\n",
    "\n",
    "$$P(T_{new}=c|X,t) = 0.5 $$\n",
    "\n",
    "$$P(T_{new} = c | x_{new}, X, t) = \\frac{p(x_{new}|T_{new}=c,X,t)0.5}{p(x_{new}|T_{new}=-1,X,t)0.5 + p(x_{new}|T_{new}=1,X,t)0.5} = $$\n",
    "\n",
    "$$ P(T_{new} = c | x_{new}, X, t) = \\frac{p(x_{new}|T_{new}=c,X,t)}{p(x_{new}|T_{new}=-1,X,t) + p(x_{new}|T_{new}=1,X,t)}$$\n",
    "\n",
    "Where: \n",
    "$$p(x_{new}|T_{new}=c,X,t) = N(\\mu_{c}, \\Sigma_{c})$$\n",
    "$$ \\mu_{c}= \\frac{1}{N_{c}}\\sum^{N_{c}}_{n=1}X_{n}$$\n",
    "$$\\Sigma_{c} = \\frac{1}{N_{c}}\\sum^{N_{c}}_{n=1}(X_{n}-\\mu_{c})(X_{n}-\\mu_{c})^T $$\n",
    "\n",
    "B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3iVQypENwJQ"
   },
   "source": [
    "## 3. [SVM, 5 points]\n",
    "\n",
    "### a (2 pts)\n",
    "\n",
    "Consider a (hard margin) SVM with the following training points from\n",
    "two classes:\n",
    "\\begin{eqnarray}\n",
    "+1: &(2,2), (4,4), (4,0) \\nonumber \\\\\n",
    "-1: &(0,0), (2,0), (0,2) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "1. Plot these six training points, and construct, by inspection, the\n",
    "weight vector for the optimal hyperplane. **(1 pt)**\n",
    "\n",
    "2. In your solution, specify the hyperplane in terms of w and b such that $w_1 x_1 + w_2 x_2 + b =0$. Calculate the margin, i.e. $2\\gamma$, where $\\gamma$ is the\n",
    "distance from the hyperplane to its closest data point. (Hint: It may be useful to recall that the distance of a point $(a_1,a_2)$ from the line $w_1x_1 + w_2x_2 + b = 0$ is $|w_1a_1 + w_2a_2 + b|/\\sqrt{w_1^2 + w_2^2}$.) **(1 pt)**\n",
    "\n",
    "### b (3 pts)\n",
    "\n",
    "Consider the same problem from above.\n",
    "\n",
    "1. Write the primal formulation of the SVM **for this specific example** i.e. you have to specialise the general formulation for the set of inputs given. **(1 pt)**\n",
    "\n",
    "2. Write the dual formulation **for this specific**. Give the optimal dual solution, comment on support vectors. **(2 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [2,4,4,0,2,0]\n",
    "y = [2,4,0,0,0,2]\n",
    "label = [1, 1, 1, -1, -1, -1]\n",
    "colors = ['red','blue']\n",
    "#plt.legend(label)\n",
    "\n",
    "plt.scatter(x, y, c=label, cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZWt7B2y1Qqx"
   },
   "source": [
    "### Your answer here:\n",
    "A)\n",
    "1) w1 = 1, w2 = 1\n",
    "\n",
    "2) For the line equation we'll use two points (1,2) and (3,0) which are in the middle between support vectors:\n",
    "$$ P = (1,2) ; Q = (3,0)$$\n",
    "$$(2-0)x_{1} + (3-1)x_{2} + (1*0 - 3*2) = 2x_{1} + 2x_{2} - 6 = x_{1} + x_{2} - 3 = 0$$\n",
    "\n",
    "Here we also confirm our weight vector $w_{1} = 1$ and $w_{2} = 1$ we visually defined in 1)\n",
    "\n",
    "Using points (0,2), (2,0), (2,2) and (4,0):\n",
    "\n",
    "$$\\gamma = \\frac{|x_{1}w_{1} + x_{2}w_{2} +b|}{\\sqrt{w_{1}^2 + w_{2}^2}}$$\n",
    "\n",
    "$$\\gamma = \\frac{0 + 2 - 3}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}}$$\n",
    "$$\\gamma = \\frac{2 + 0 - 3}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}}$$\n",
    "$$\\gamma = \\frac{2 + 2 - 3}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}}$$\n",
    "$$\\gamma = \\frac{4 + 0 - 3}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}}$$\n",
    "\n",
    "$$ \\gamma =  \\frac{1}{\\sqrt{2}} $$\n",
    "$$ 2\\gamma =  \\frac{2}{\\sqrt{2}} =  \\sqrt{2}$$\n",
    "\n",
    "B)\n",
    "Primal:\n",
    "$$argmin_{w} \\frac{1}{2}||w||^2 $$\n",
    "subject to $$ t_{n}(w^Tx_{n} + b) \\ge 1  \\forall n$$\n",
    "Making it specific for our problem:\n",
    "\n",
    "$$argmin_{w} \\frac{1}{2}||w||^2 $$\n",
    "Subject to: \n",
    "$$1(2w_{1} + 2w_{2} + b) \\ge 1$$\n",
    "$$1(4w_{1} + 4w_{2} + b) \\ge 1$$\n",
    "$$1(4w_{1} + b) \\ge 1$$\n",
    "$$-1(b) \\ge 1$$\n",
    "$$-1(2w_{1} + b) \\ge 1$$\n",
    "$$-1(2w_{2} + b) \\ge 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhHF2_-R00si"
   },
   "source": [
    "# Practical Question\n",
    "## 4. Logistic Regression (5 pts)\n",
    "### Customer churn with Logistic Regression\n",
    "A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out who is leaving and why.\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n",
    "This data set provides information to help you predict what behavior will help you to retain customers. You can analyse all relevant customer data and develop focused customer retention programs.\n",
    "The dataset includes information about:\n",
    "*   Customers who left within the last month – the column is called Churn.\n",
    "*   Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies.\n",
    "*   Customer account information – how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges.\n",
    "*   Demographic info about customers – gender, age range, and if they have partners and dependents.\n",
    "We will help you load and visualise the dataset as well as the preprocessing, you need to build up your logistic regression model step by step and do the prediction.\n",
    "*   **Remember, you are not allowed to use sklearn in modelling and predicting, you have to fill your code in the skeleton.** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR5cEzzzOVba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "\n",
       "   longmon  ...  pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "0     4.40  ...    1.0       0.0       1.0     1.0    0.0    1.482    3.033   \n",
       "1     9.45  ...    0.0       0.0       0.0     0.0    0.0    2.246    3.240   \n",
       "2     6.30  ...    0.0       0.0       0.0     1.0    0.0    1.841    3.240   \n",
       "3     6.05  ...    1.0       1.0       1.0     1.0    1.0    1.800    3.807   \n",
       "4     7.10  ...    0.0       0.0       1.0     1.0    0.0    1.960    3.091   \n",
       "\n",
       "   lninc  custcat  churn  \n",
       "0  4.913      4.0    1.0  \n",
       "1  3.497      1.0    1.0  \n",
       "2  3.401      3.0    0.0  \n",
       "3  4.331      4.0    0.0  \n",
       "4  4.382      3.0    0.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the dataset and read it\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "import urllib\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/BruceZHANG6/Imagesforuse/master/ChurnData.csv', 'ChurnData.csv')\n",
    "except urllib.error.HTTPError as ex:\n",
    "    print('Problem:', ex)\n",
    "    \n",
    "churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2gDgP27WNFV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (7, 160) (1, 160)\n",
      "Test set: (7, 40) (1, 40)\n"
     ]
    }
   ],
   "source": [
    "## Data pre-processing and selection\n",
    "## Train/Test dataset split\n",
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "churn_df['churn'] = churn_df['churn'].astype('int')\n",
    "churn_df.head()\n",
    "\n",
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n",
    "y = np.asarray(churn_df['churn'])\n",
    "y = np.reshape(y, (np.asarray(churn_df['churn']).shape[0], 1))\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "y_train=y_train.T\n",
    "y_test=y_test.T\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMzOgQsCoJSN"
   },
   "source": [
    "**Hints**:\n",
    "- You compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ .\n",
    "- You compute activation $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$.\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$.\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "\n",
    "- You write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.\n",
    "- In prediction, you calculate $\\hat{Y} = A = \\sigma(w^T X + b)$.\n",
    "- You may use np.exp, np.log(), np.dot(), etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCZ4BQ-uW6hg"
   },
   "outputs": [],
   "source": [
    "## Modeling and predicting\n",
    "\n",
    "# GRADED FUNCTION: sigmoid\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Return: s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1/(1+ np.exp(-z)) # Your code here\n",
    "    \n",
    "    return s\n",
    "  \n",
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized to 0\n",
    "    \"\"\"    \n",
    "    w = np.zeros((dim, 1)) # Your code here\n",
    "    b = 0 # Your code here\n",
    "   \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "  \n",
    "# GRADED FUNCTION: grad_cost\n",
    "def grad_cost(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data of size (number of features, number of examples)\n",
    "    Y -- true \"label\" vector\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\" \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A    = sigmoid(np.dot(np.transpose(w), X) +b) # Your code here\n",
    "    cost = -1/m * sum(Y[0,i] * np.log(A[0,i]) + (1-Y[0,i]) * np.log(1 - A[0,i]) for i in range(m)) # Your code here\n",
    "    dw   =  1/m * np.dot(X, np.transpose(A-Y)) # Your code here\n",
    "    db   =  1/m * sum(A[0,i] - Y[0,i] for i in range(m)) # Your code here\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    " # GRADED FUNCTION: optimize\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = grad_cost(w,b,X,Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = w - learning_rate*dw # Your code here\n",
    "        b = b - learning_rate*db # Your code here\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)     \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}   \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "  \n",
    "# GRADED FUNCTION: predict\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = sigmoid(np.dot(np.transpose(w), X) + b) # Your code here\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        if A[0,i]<=0.5:\n",
    "            Y_prediction[0,i]=0;\n",
    "        if A[0,i]>0.5:\n",
    "            Y_prediction[0,i]=1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "  \n",
    "# GRADED FUNCTION: model\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    Returns: d -- dictionary containing information about the model.\n",
    "    \"\"\"    \n",
    "    # initialize parameters with zeros\n",
    "    w, b = initialize_with_zeros(X_train.shape[0]) #Your code here\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) #Your code here\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "   \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wl7F4KSwZo9T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.630203\n",
      "Cost after iteration 200: 0.590523\n",
      "Cost after iteration 300: 0.563211\n",
      "Cost after iteration 400: 0.543095\n",
      "Cost after iteration 500: 0.527566\n",
      "Cost after iteration 600: 0.515188\n",
      "Cost after iteration 700: 0.505098\n",
      "Cost after iteration 800: 0.496739\n",
      "Cost after iteration 900: 0.489725\n",
      "Cost after iteration 1000: 0.483781\n",
      "Cost after iteration 1100: 0.478702\n",
      "Cost after iteration 1200: 0.474330\n",
      "Cost after iteration 1300: 0.470543\n",
      "Cost after iteration 1400: 0.467245\n",
      "Cost after iteration 1500: 0.464357\n",
      "Cost after iteration 1600: 0.461818\n",
      "Cost after iteration 1700: 0.459574\n",
      "Cost after iteration 1800: 0.457585\n",
      "Cost after iteration 1900: 0.455815\n",
      "Cost after iteration 2000: 0.454234\n",
      "Cost after iteration 2100: 0.452818\n",
      "Cost after iteration 2200: 0.451546\n",
      "Cost after iteration 2300: 0.450401\n",
      "Cost after iteration 2400: 0.449367\n",
      "Cost after iteration 2500: 0.448431\n",
      "Cost after iteration 2600: 0.447583\n",
      "Cost after iteration 2700: 0.446812\n",
      "Cost after iteration 2800: 0.446110\n",
      "Cost after iteration 2900: 0.445470\n",
      "Cost after iteration 3000: 0.444886\n",
      "Cost after iteration 3100: 0.444351\n",
      "Cost after iteration 3200: 0.443862\n",
      "Cost after iteration 3300: 0.443413\n",
      "Cost after iteration 3400: 0.443000\n",
      "Cost after iteration 3500: 0.442621\n",
      "Cost after iteration 3600: 0.442272\n",
      "Cost after iteration 3700: 0.441950\n",
      "Cost after iteration 3800: 0.441654\n",
      "Cost after iteration 3900: 0.441380\n",
      "Cost after iteration 4000: 0.441127\n",
      "Cost after iteration 4100: 0.440893\n",
      "Cost after iteration 4200: 0.440676\n",
      "Cost after iteration 4300: 0.440475\n",
      "Cost after iteration 4400: 0.440289\n",
      "Cost after iteration 4500: 0.440117\n",
      "Cost after iteration 4600: 0.439957\n",
      "Cost after iteration 4700: 0.439808\n",
      "Cost after iteration 4800: 0.439670\n",
      "Cost after iteration 4900: 0.439541\n",
      "Cost after iteration 5000: 0.439422\n",
      "Cost after iteration 5100: 0.439310\n",
      "Cost after iteration 5200: 0.439207\n",
      "Cost after iteration 5300: 0.439110\n",
      "Cost after iteration 5400: 0.439020\n",
      "Cost after iteration 5500: 0.438936\n",
      "Cost after iteration 5600: 0.438858\n",
      "Cost after iteration 5700: 0.438785\n",
      "Cost after iteration 5800: 0.438717\n",
      "Cost after iteration 5900: 0.438653\n",
      "Cost after iteration 6000: 0.438593\n",
      "Cost after iteration 6100: 0.438538\n",
      "Cost after iteration 6200: 0.438486\n",
      "Cost after iteration 6300: 0.438437\n",
      "Cost after iteration 6400: 0.438392\n",
      "Cost after iteration 6500: 0.438349\n",
      "Cost after iteration 6600: 0.438309\n",
      "Cost after iteration 6700: 0.438272\n",
      "Cost after iteration 6800: 0.438237\n",
      "Cost after iteration 6900: 0.438204\n",
      "Cost after iteration 7000: 0.438173\n",
      "Cost after iteration 7100: 0.438144\n",
      "Cost after iteration 7200: 0.438118\n",
      "Cost after iteration 7300: 0.438092\n",
      "Cost after iteration 7400: 0.438068\n",
      "Cost after iteration 7500: 0.438046\n",
      "Cost after iteration 7600: 0.438025\n",
      "Cost after iteration 7700: 0.438006\n",
      "Cost after iteration 7800: 0.437987\n",
      "Cost after iteration 7900: 0.437970\n",
      "Cost after iteration 8000: 0.437954\n",
      "Cost after iteration 8100: 0.437939\n",
      "Cost after iteration 8200: 0.437924\n",
      "Cost after iteration 8300: 0.437911\n",
      "Cost after iteration 8400: 0.437898\n",
      "Cost after iteration 8500: 0.437886\n",
      "Cost after iteration 8600: 0.437875\n",
      "Cost after iteration 8700: 0.437865\n",
      "Cost after iteration 8800: 0.437855\n",
      "Cost after iteration 8900: 0.437845\n",
      "Cost after iteration 9000: 0.437837\n",
      "Cost after iteration 9100: 0.437828\n",
      "Cost after iteration 9200: 0.437821\n",
      "Cost after iteration 9300: 0.437813\n",
      "Cost after iteration 9400: 0.437807\n",
      "Cost after iteration 9500: 0.437800\n",
      "Cost after iteration 9600: 0.437794\n",
      "Cost after iteration 9700: 0.437788\n",
      "Cost after iteration 9800: 0.437783\n",
      "Cost after iteration 9900: 0.437778\n",
      "Cost after iteration 10000: 0.437773\n",
      "Cost after iteration 10100: 0.437769\n",
      "Cost after iteration 10200: 0.437765\n",
      "Cost after iteration 10300: 0.437761\n",
      "Cost after iteration 10400: 0.437757\n",
      "Cost after iteration 10500: 0.437753\n",
      "Cost after iteration 10600: 0.437750\n",
      "Cost after iteration 10700: 0.437747\n",
      "Cost after iteration 10800: 0.437744\n",
      "Cost after iteration 10900: 0.437741\n",
      "Cost after iteration 11000: 0.437739\n",
      "Cost after iteration 11100: 0.437736\n",
      "Cost after iteration 11200: 0.437734\n",
      "Cost after iteration 11300: 0.437732\n",
      "Cost after iteration 11400: 0.437729\n",
      "Cost after iteration 11500: 0.437728\n",
      "Cost after iteration 11600: 0.437726\n",
      "Cost after iteration 11700: 0.437724\n",
      "Cost after iteration 11800: 0.437722\n",
      "Cost after iteration 11900: 0.437721\n",
      "Cost after iteration 12000: 0.437719\n",
      "Cost after iteration 12100: 0.437718\n",
      "Cost after iteration 12200: 0.437717\n",
      "Cost after iteration 12300: 0.437715\n",
      "Cost after iteration 12400: 0.437714\n",
      "Cost after iteration 12500: 0.437713\n",
      "Cost after iteration 12600: 0.437712\n",
      "Cost after iteration 12700: 0.437711\n",
      "Cost after iteration 12800: 0.437710\n",
      "Cost after iteration 12900: 0.437709\n",
      "Cost after iteration 13000: 0.437709\n",
      "Cost after iteration 13100: 0.437708\n",
      "Cost after iteration 13200: 0.437707\n",
      "Cost after iteration 13300: 0.437706\n",
      "Cost after iteration 13400: 0.437706\n",
      "Cost after iteration 13500: 0.437705\n",
      "Cost after iteration 13600: 0.437705\n",
      "Cost after iteration 13700: 0.437704\n",
      "Cost after iteration 13800: 0.437704\n",
      "Cost after iteration 13900: 0.437703\n",
      "Cost after iteration 14000: 0.437703\n",
      "Cost after iteration 14100: 0.437702\n",
      "Cost after iteration 14200: 0.437702\n",
      "Cost after iteration 14300: 0.437701\n",
      "Cost after iteration 14400: 0.437701\n",
      "Cost after iteration 14500: 0.437701\n",
      "Cost after iteration 14600: 0.437700\n",
      "Cost after iteration 14700: 0.437700\n",
      "Cost after iteration 14800: 0.437700\n",
      "Cost after iteration 14900: 0.437699\n",
      "Cost after iteration 15000: 0.437699\n",
      "Cost after iteration 15100: 0.437699\n",
      "Cost after iteration 15200: 0.437699\n",
      "Cost after iteration 15300: 0.437699\n",
      "Cost after iteration 15400: 0.437698\n",
      "Cost after iteration 15500: 0.437698\n",
      "Cost after iteration 15600: 0.437698\n",
      "Cost after iteration 15700: 0.437698\n",
      "Cost after iteration 15800: 0.437698\n",
      "Cost after iteration 15900: 0.437697\n",
      "Cost after iteration 16000: 0.437697\n",
      "Cost after iteration 16100: 0.437697\n",
      "Cost after iteration 16200: 0.437697\n",
      "Cost after iteration 16300: 0.437697\n",
      "Cost after iteration 16400: 0.437697\n",
      "Cost after iteration 16500: 0.437697\n",
      "Cost after iteration 16600: 0.437697\n",
      "Cost after iteration 16700: 0.437696\n",
      "Cost after iteration 16800: 0.437696\n",
      "Cost after iteration 16900: 0.437696\n",
      "Cost after iteration 17000: 0.437696\n",
      "Cost after iteration 17100: 0.437696\n",
      "Cost after iteration 17200: 0.437696\n",
      "Cost after iteration 17300: 0.437696\n",
      "Cost after iteration 17400: 0.437696\n",
      "Cost after iteration 17500: 0.437696\n",
      "Cost after iteration 17600: 0.437696\n",
      "Cost after iteration 17700: 0.437696\n",
      "Cost after iteration 17800: 0.437696\n",
      "Cost after iteration 17900: 0.437696\n",
      "Cost after iteration 18000: 0.437696\n",
      "Cost after iteration 18100: 0.437696\n",
      "Cost after iteration 18200: 0.437695\n",
      "Cost after iteration 18300: 0.437695\n",
      "Cost after iteration 18400: 0.437695\n",
      "Cost after iteration 18500: 0.437695\n",
      "Cost after iteration 18600: 0.437695\n",
      "Cost after iteration 18700: 0.437695\n",
      "Cost after iteration 18800: 0.437695\n",
      "Cost after iteration 18900: 0.437695\n",
      "Cost after iteration 19000: 0.437695\n",
      "Cost after iteration 19100: 0.437695\n",
      "Cost after iteration 19200: 0.437695\n",
      "Cost after iteration 19300: 0.437695\n",
      "Cost after iteration 19400: 0.437695\n",
      "Cost after iteration 19500: 0.437695\n",
      "Cost after iteration 19600: 0.437695\n",
      "Cost after iteration 19700: 0.437695\n",
      "Cost after iteration 19800: 0.437695\n",
      "Cost after iteration 19900: 0.437695\n",
      "train accuracy: 77.5 %\n",
      "test accuracy: 72.5 %\n"
     ]
    }
   ],
   "source": [
    "## The train accuracy and test accuracy\n",
    "## Feel free to change the hyperparameters\n",
    "d = model(X_train, y_train, X_test, y_test, num_iterations = 20000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_2019.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
